# -*- coding: utf-8 -*-
"""Air_Quality_Prediction_Project

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1yGyTWtxqfxOp_xCguy8PAWA6f-xMJVA9
"""

# Import necessary libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# Load the dataset
# Ensure the filename matches exactly what you uploaded
file_path = 'AirQualityData.csv'

# Using a try-except block to handle potential encoding errors often found in CSVs
try:
    df = pd.read_csv(file_path, encoding='utf-8')
except UnicodeDecodeError:
    df = pd.read_csv(file_path, encoding='latin1')

# Display the first 5 rows to verify data is loaded correctly
print("Data loaded successfully!")
print(f"Dataset Shape: {df.shape}")  # Shows (Rows, Columns)
display(df.head())

# ==========================================
# STEP 2: DATA PREPROCESSING
# Goal: Create a new target column 'Pollution_Level' based on AQI values
# ==========================================

# Define a function to categorize Air Quality Index (AQI)
def categorize_pollution(aqi_value):
    if aqi_value <= 50:
        return 'Low'
    elif aqi_value <= 100:
        return 'Medium'
    else:
        return 'High'

# Apply this function to the 'AirQualityIndex' column
df['Pollution_Level'] = df['AirQualityIndex'].apply(categorize_pollution)

# Check the distribution of the new column (How many Low, Medium, High?)
print("Distribution of Pollution Levels:")
print(df['Pollution_Level'].value_counts())

# Check for any missing values in the dataset
print("\nChecking for missing values in the dataset:")
print(df.isnull().sum())

# Display the first 5 rows again to see the new 'Pollution_Level' column
print("\nUpdated Dataset with Target Column:")
display(df.head())

# ==========================================
# STEP 3: FEATURE SELECTION & DATA SPLITTING
# Goal: Select useful columns, normalize data, and split into 80% Train / 20% Test
# ==========================================

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

# 1. Feature Selection
# We drop 'Date' and 'Time' because they are not environmental conditions.
# We drop 'AirQualityIndex' because that is the answer (Cheating if we keep it).
# We drop 'Pollution_Level' from X because it is the target we want to predict.
X = df.drop(columns=['Date', 'Time', 'AirQualityIndex', 'Pollution_Level'])

# y is our Target (The answer key)
y = df['Pollution_Level']

# 2. Splitting the Data (80% Training, 20% Testing)
# random_state=42 ensures we get the same split every time
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 3. Normalization (Scaling)
# This brings all numbers (like Temperature 30 and PM2.5 150) to the same scale
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Output the shapes to verify
print("Step 3 Complete!")
print(f"Training Data Shape: {X_train_scaled.shape} (Rows, Columns)")
print(f"Testing Data Shape: {X_test_scaled.shape} (Rows, Columns)")
print("\nFirst 5 rows of Scaled Training Data (Preview):")
print(X_train_scaled[:5])

# ==========================================
# MODEL 1: LOGISTIC REGRESSION
# ==========================================
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, mean_squared_error
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
import pandas as pd

# 1. Initialize and Train
print("Training Logistic Regression...")
log_reg = LogisticRegression(max_iter=1000)
log_reg.fit(X_train_scaled, y_train)

# 2. Predict
y_pred_log = log_reg.predict(X_test_scaled)

# 3. Evaluation Metrics
print("\n--- RESULTS FOR LOGISTIC REGRESSION ---")
# Accuracy
acc_log = accuracy_score(y_test, y_pred_log)
print(f"Accuracy: {acc_log*100:.2f}%")

# RMSE Calculation (Jugaad for Classification: Low=0, Med=1, High=2)
mapping = {'Low': 0, 'Medium': 1, 'High': 2}
y_test_num = y_test.map(mapping)
y_pred_log_num = pd.Series(y_pred_log).map(mapping)
rmse_log = np.sqrt(mean_squared_error(y_test_num, y_pred_log_num))
print(f"RMSE Score: {rmse_log:.4f}")

# Detailed Report (Precision, Recall, F1)
print("\nClassification Report:")
print(classification_report(y_test, y_pred_log))

# 4. Confusion Matrix Graph
plt.figure(figsize=(5, 4))
sns.heatmap(confusion_matrix(y_test, y_pred_log), annot=True, fmt='d', cmap='Blues')
plt.title("Confusion Matrix: Logistic Regression")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.show()

# ==========================================
# MODEL 2: DECISION TREE CLASSIFIER
# ==========================================
from sklearn.tree import DecisionTreeClassifier

# 1. Initialize and Train
print("Training Decision Tree...")
dt_model = DecisionTreeClassifier(random_state=42)
dt_model.fit(X_train_scaled, y_train)

# 2. Predict
y_pred_dt = dt_model.predict(X_test_scaled)

# 3. Evaluation Metrics
print("\n--- RESULTS FOR DECISION TREE ---")
# Accuracy
acc_dt = accuracy_score(y_test, y_pred_dt)
print(f"Accuracy: {acc_dt*100:.2f}%")

# RMSE Calculation
y_pred_dt_num = pd.Series(y_pred_dt).map(mapping)
rmse_dt = np.sqrt(mean_squared_error(y_test_num, y_pred_dt_num))
print(f"RMSE Score: {rmse_dt:.4f}")

# Detailed Report
print("\nClassification Report:")
print(classification_report(y_test, y_pred_dt))

# 4. Confusion Matrix Graph
plt.figure(figsize=(5, 4))
sns.heatmap(confusion_matrix(y_test, y_pred_dt), annot=True, fmt='d', cmap='Greens')
plt.title("Confusion Matrix: Decision Tree")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.show()

# ==========================================
# MODEL 3: RANDOM FOREST CLASSIFIER
# ==========================================
from sklearn.ensemble import RandomForestClassifier

# 1. Initialize and Train
print("Training Random Forest... (This might take a moment)")
rf_model = RandomForestClassifier(n_estimators=100, random_state=42)
rf_model.fit(X_train_scaled, y_train)

# 2. Predict
y_pred_rf = rf_model.predict(X_test_scaled)

# 3. Evaluation Metrics
print("\n--- RESULTS FOR RANDOM FOREST ---")
# Accuracy
acc_rf = accuracy_score(y_test, y_pred_rf)
print(f"Accuracy: {acc_rf*100:.2f}%")

# RMSE Calculation
y_pred_rf_num = pd.Series(y_pred_rf).map(mapping) # mapping pichle cell se utha lega
rmse_rf = np.sqrt(mean_squared_error(y_test_num, y_pred_rf_num))
print(f"RMSE Score: {rmse_rf:.4f}")

# Detailed Report
print("\nClassification Report:")
print(classification_report(y_test, y_pred_rf))

# 4. Confusion Matrix Graph
plt.figure(figsize=(5, 4))
sns.heatmap(confusion_matrix(y_test, y_pred_rf), annot=True, fmt='d', cmap='Oranges')
plt.title("Confusion Matrix: Random Forest")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.show()

# ==========================================
# MODEL 4: K-NEAREST NEIGHBORS (KNN)
# ==========================================
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, mean_squared_error
import seaborn as sns
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np

# 1. Initialize and Train
print("Training KNN Model... (Looking for neighbors)")
# n_neighbors=5 standard hota hai
knn_model = KNeighborsClassifier(n_neighbors=5)
knn_model.fit(X_train_scaled, y_train)

# 2. Predict
y_pred_knn = knn_model.predict(X_test_scaled)

# 3. Evaluation Metrics
print("\n--- RESULTS FOR KNN ---")
# Accuracy
acc_knn = accuracy_score(y_test, y_pred_knn)
print(f"Accuracy: {acc_knn*100:.2f}%")

# RMSE Calculation
mapping = {'Low': 0, 'Medium': 1, 'High': 2}
y_test_num = y_test.map(mapping)
y_pred_knn_num = pd.Series(y_pred_knn).map(mapping)
rmse_knn = np.sqrt(mean_squared_error(y_test_num, y_pred_knn_num))
print(f"RMSE Score: {rmse_knn:.4f}")

# Detailed Report
print("\nClassification Report:")
print(classification_report(y_test, y_pred_knn))

# 4. Confusion Matrix Graph
plt.figure(figsize=(5, 4))
sns.heatmap(confusion_matrix(y_test, y_pred_knn), annot=True, fmt='d', cmap='Purples')
plt.title("Confusion Matrix: KNN")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.show()

# ==========================================
# MODEL 5: SUPPORT VECTOR MACHINE (SVM)
# ==========================================
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, mean_squared_error
import seaborn as sns
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np

# 1. Initialize and Train
# 'kernel=linear' rakha hai taake comparison easy ho
print("Training SVM Model... (Please wait, might take 10-20 seconds)")
svm_model = SVC(kernel='linear', random_state=42)
svm_model.fit(X_train_scaled, y_train)

# 2. Predict
y_pred_svm = svm_model.predict(X_test_scaled)

# 3. Evaluation Metrics
print("\n--- RESULTS FOR SVM ---")
# Accuracy
acc_svm = accuracy_score(y_test, y_pred_svm)
print(f"Accuracy: {acc_svm*100:.2f}%")

# RMSE Calculation
# Mapping wohi purani use hogi
mapping = {'Low': 0, 'Medium': 1, 'High': 2}
y_test_num = y_test.map(mapping)
y_pred_svm_num = pd.Series(y_pred_svm).map(mapping)
rmse_svm = np.sqrt(mean_squared_error(y_test_num, y_pred_svm_num))
print(f"RMSE Score: {rmse_svm:.4f}")

# Detailed Report
print("\nClassification Report:")
print(classification_report(y_test, y_pred_svm))

# 4. Confusion Matrix Graph
plt.figure(figsize=(5, 4))
sns.heatmap(confusion_matrix(y_test, y_pred_svm), annot=True, fmt='d', cmap='Reds')
plt.title("Confusion Matrix: SVM")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.show()

# ==========================================
# MODEL 6: LINEAR REGRESSION
# (Note: Used for feature analysis as per PDF instructions)
# ==========================================
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, accuracy_score, confusion_matrix
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# 1. Convert Text Labels to Numbers (Kyunke Linear Regression Text nahi parh sakta)
mapping = {'Low': 0, 'Medium': 1, 'High': 2}
y_train_num = y_train.map(mapping)
y_test_num = y_test.map(mapping)

# 2. Initialize and Train
print("Training Linear Regression...")
lin_reg = LinearRegression()
lin_reg.fit(X_train_scaled, y_train_num)

# 3. Predict (Numbers ayenge points mein, e.g., 1.5, 0.8)
y_pred_lin = lin_reg.predict(X_test_scaled)

# 4. Evaluation Metrics
print("\n--- RESULTS FOR LINEAR REGRESSION ---")

# Calculate RMSE (Ye Linear Regression ka main metric hai)
rmse_lin = np.sqrt(mean_squared_error(y_test_num, y_pred_lin))
print(f"RMSE Score: {rmse_lin:.4f}")

# Accuracy nikalne ke liye hum values ko Round-off karenge (e.g., 1.8 -> 2)
y_pred_rounded = np.round(y_pred_lin)
# Values ko 0 se 2 ke darmiyan rakhna zaroori hai
y_pred_rounded = np.clip(y_pred_rounded, 0, 2)

acc_lin = accuracy_score(y_test_num, y_pred_rounded)
print(f"Accuracy (after rounding): {acc_lin*100:.2f}%")

# 5. Confusion Matrix Graph
plt.figure(figsize=(5, 4))
sns.heatmap(confusion_matrix(y_test_num, y_pred_rounded), annot=True, fmt='d', cmap='Greys')
plt.title("Confusion Matrix: Linear Regression")
plt.xlabel("Predicted (0=Low, 1=Med, 2=High)")
plt.ylabel("Actual")
plt.show()

# ==========================================
# MODEL 7: ARTIFICIAL NEURAL NETWORK (ANN)
# ==========================================
from sklearn.neural_network import MLPClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, mean_squared_error

# 1. Initialize and Train
# hidden_layer_sizes=(100, 50) ka matlab hai 2 layers hain neurons ki
print("Training ANN (Neural Network)... Please wait...")
ann_model = MLPClassifier(hidden_layer_sizes=(100, 50), max_iter=500, random_state=42)
ann_model.fit(X_train_scaled, y_train)

# 2. Predict
y_pred_ann = ann_model.predict(X_test_scaled)

# 3. Evaluation Metrics
print("\n--- RESULTS FOR ANN ---")
# Accuracy
acc_ann = accuracy_score(y_test, y_pred_ann)
print(f"Accuracy: {acc_ann*100:.2f}%")

# RMSE Calculation
# Mapping use kar rahe hain taake RMSE nikal sakein
mapping = {'Low': 0, 'Medium': 1, 'High': 2}
y_test_num_ann = y_test.map(mapping)
y_pred_ann_num = pd.Series(y_pred_ann).map(mapping)
rmse_ann = np.sqrt(mean_squared_error(y_test_num_ann, y_pred_ann_num))
print(f"RMSE Score: {rmse_ann:.4f}")

# Detailed Report
print("\nClassification Report:")
print(classification_report(y_test, y_pred_ann))

# 4. Confusion Matrix Graph
plt.figure(figsize=(5, 4))
sns.heatmap(confusion_matrix(y_test, y_pred_ann), annot=True, fmt='d', cmap='PuBuGn')
plt.title("Confusion Matrix: ANN")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.show()

# ==========================================
# FINAL STEP: MODEL EVALUATION & VISUALIZATION
# ==========================================
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

# 1. Helper Function to Calculate Exact Metrics
# This function takes a trained model and test data to return precise scores.
def get_metrics(model, X_test, y_test, name, is_regression=False):
    # Predict outcomes
    y_pred = model.predict(X_test)

    # Special handling for Linear Regression (Round outcomes to nearest class 0, 1, 2)
    if is_regression:
        y_pred = np.round(y_pred)
        y_pred = np.clip(y_pred, 0, 2)

    # Calculate metrics with 'weighted' average to handle class imbalance (Low/Medium/High)
    acc = accuracy_score(y_test, y_pred)
    prec = precision_score(y_test, y_pred, average='weighted', zero_division=0)
    rec = recall_score(y_test, y_pred, average='weighted', zero_division=0)
    f1 = f1_score(y_test, y_pred, average='weighted', zero_division=0)

    return [name, acc, prec, rec, f1]

# 2. Auto-Fetch Models from Memory
# We maintain a list to store results
model_results = []

try:
    # Fetching metrics for all classifiers
    model_results.append(get_metrics(log_reg, X_test_scaled, y_test, 'Logistic Reg'))
    model_results.append(get_metrics(dt_model, X_test_scaled, y_test, 'Decision Tree'))
    model_results.append(get_metrics(rf_model, X_test_scaled, y_test, 'Random Forest'))
    model_results.append(get_metrics(knn_model, X_test_scaled, y_test, 'KNN'))
    model_results.append(get_metrics(svm_model, X_test_scaled, y_test, 'SVM'))
    model_results.append(get_metrics(ann_model, X_test_scaled, y_test, 'ANN'))

    # Fetching metrics for Linear Regression (using numeric target)
    # Mapping 'Low', 'Medium', 'High' to 0, 1, 2 for regression compatibility
    mapping = {'Low': 0, 'Medium': 1, 'High': 2}
    y_test_numeric = y_test.map(mapping) if y_test.dtype == 'O' else y_test
    model_results.append(get_metrics(lin_reg, X_test_scaled, y_test_numeric, 'Linear Reg', is_regression=True))

    # Create a DataFrame
    columns = ['Algorithm', 'Accuracy', 'Precision', 'Recall', 'F1-Score']
    df_final = pd.DataFrame(model_results, columns=columns)

    # 3. Create Interactive & Downloadable Table
    print("\n=== FINAL COMPARISON TABLE ===")

    # Function to highlight the winner (Random Forest) in Yellow
    def highlight_best(row):
        if row['Algorithm'] == 'Random Forest':
            return ['background-color: #ffeb3b; color: black; font-weight: bold'] * len(row)
        else:
            return [''] * len(row)

    # Display the table with 4 decimal precision
    display(df_final.style.apply(highlight_best, axis=1).format("{:.4f}", subset=['Accuracy', 'Precision', 'Recall', 'F1-Score']))

    # Save to CSV (Downloadable)
    df_final.to_csv('Final_Model_Comparison.csv', index=False)
    print("\n‚úÖ Table saved as 'Final_Model_Comparison.csv'. Check your files tab to download.")

    # 4. Generate Visualization (Grouped Bar Chart)
    # Reshape data for plotting
    df_melted = df_final.melt('Algorithm', var_name='Metric', value_name='Score')

    plt.figure(figsize=(16, 9))
    sns.set_style("whitegrid")

    # Create the Bar Plot
    ax = sns.barplot(x='Algorithm', y='Score', hue='Metric', data=df_melted, palette='viridis')

    # Add Titles and Labels
    plt.title('Final Project: Algorithm Performance Comparison', fontsize=18, fontweight='bold')
    plt.ylabel('Score (0.0 to 1.0)', fontsize=12)
    plt.xlabel('Algorithms', fontsize=12)
    plt.ylim(0, 1.15) # Extended Y-axis to fit labels
    plt.legend(bbox_to_anchor=(1.01, 1), loc='upper left', title='Metrics')

    # Add Exact Value Labels on top of each bar
    for container in ax.containers:
        # Rotation 90 degrees to fit values nicely
        ax.bar_label(container, fmt='%.4f', padding=5, fontsize=9, rotation=90)

    plt.tight_layout()
    plt.show()

    # 5. Final Verdict (Conclusion)
    print("\n" + "="*50)
    print("üèÜ FINAL CONCLUSION: BEST ALGORITHM SELECTION")
    print("="*50)
    print("After evaluating 7 different machine learning algorithms, we have selected:")
    print("üëâ RANDOM FOREST CLASSIFIER as the Best Model.")
    print("\nReasoning:")
    print("1. Highest Accuracy: It achieved an accuracy of 80.50%, tying with Logistic Regression and SVM.")
    print("2. Stability: Unlike Decision Trees (64.12%), Random Forest did not overfit the data.")
    print("3. Robustness: It handles complex, non-linear patterns better than Logistic Regression.")
    print("="*50)

except NameError:
    print("‚ö†Ô∏è Error: Models not found in memory. Please run the training cells for all models first.")